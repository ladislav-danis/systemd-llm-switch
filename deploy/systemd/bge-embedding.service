[Unit]
Description=Llama-CPP Server (BGE-M3)
# Service should start after network and nvidia services are available
After=network.target
After=network.target nvidia-persistenced.service

[Service]
# Working directory where the llama-server executable is located
WorkingDirectory=/home/spravce/llama/llama.cpp/build-cuda

# Main command to start the llama-server with various parameters
ExecStart=/home/spravce/llama/llama.cpp/build-cuda/bin/llama-server \
-hf ggml-org/bge-m3-Q8_0-GGUF:Q8_0 \
-np 1 \
--threads 3 \
--batch-size 2048 \
--ubatch-size 2048 \
--ctx-size 8192 \
-a bge-m3 \
--flash-attn on \
--n-gpu-layers 99 \
--host 0.0.0.0 \
--port 3004 \
--embedding

# Restart the service if it crashes or stops
Restart=always
# Wait 5 seconds before restarting
RestartSec=5

[Install]
# Service should be started when the system reaches multi-user target
WantedBy=multi-user.target