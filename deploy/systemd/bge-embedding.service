[Unit]
Description=Llama-CPP Server (BGE-M3)
# Service should start after network and nvidia services are available
After=network.target
After=network.target nvidia-persistenced.service

[Service]
# Working directory where the llama-server executable is located
WorkingDirectory=/path/to/your/llama/llama.cpp/build-cuda

# Main command to start the llama-server with various parameters
ExecStart=/path/to/your/llama/llama.cpp/build-cuda/bin/llama-server \
--model /path/to/your/llama/ggml-org_bge-m3-Q8_0-GGUF_bge-m3-q8_0.gguf \
-np 1 \
--threads 3 \
--batch-size 2048 \
--ubatch-size 2048 \
--ctx-size 8192 \
-a bge-m3 \
--flash-attn on \
--n-gpu-layers 99 \
--host 0.0.0.0 \
--port 3004 \
--embedding

# Restart the service if it crashes or stops
Restart=always
# Wait 5 seconds before restarting
RestartSec=5

[Install]
# Service should be started when the system reaches multi-user target
WantedBy=multi-user.target