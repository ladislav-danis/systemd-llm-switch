[Unit]
# Description of the service
Description=Llama-CPP Server (Qwen3-Coder-30B-A3B-8GB)
# Service should start after network and nvidia services are available
After=network.target
After=network.target nvidia-persistenced.service

[Service]
# Working directory where the llama-server executable is located
WorkingDirectory=/home/spravce/llama/llama.cpp/build-cuda

# Main command to start the llama-server with various parameters
ExecStart=/home/spravce/llama/llama.cpp/build-cuda/bin/llama-server \
-hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q8_K_XL \
--threads 3 \
-np 1 \
--batch-size 512 \
--ubatch-size 512 \
--ctx-size 20480 \
--cache-type-k q8_0 \
--cache-type-v q8_0 \
--jinja \
--temp 0.7 --min-p 0.01 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 \
--flash-attn on \
-a qwen3-coder-30-a3b-8gb \
--n-gpu-layers 49 \
--override-tensor ".*blk\.([4-9]|[1-3][0-9]|4[0-8])\.ffn_.*_exps.*=CPU" \
--host 0.0.0.0 \
--port 3004

# Restart the service if it crashes or stops
Restart=always
# Wait 5 seconds before restarting
RestartSec=5

[Install]
# Service should be started when the system reaches multi-user target
WantedBy=multi-user.target
