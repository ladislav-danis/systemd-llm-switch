[Unit]
# Description of the service
Description=Llama-CPP Server (Qwen3-Thinking-30B-A3B-8GB)
# Service should start after network and nvidia services are available
After=network.target
After=network.target nvidia-persistenced.service

[Service]
# Working directory where the llama-server executable is located
WorkingDirectory=/home/spravce/llama/llama.cpp/build-cuda

# Main command to start the llama-server with various parameters
ExecStart=/home/spravce/llama/llama.cpp/build-cuda/bin/llama-server \
--model /raid/spravce/llama/unsloth_Qwen3-30B-A3B-Thinking-2507-GGUF_Qwen3-30B-A3B-Thinking-2507-UD-Q8_K_XL.gguf \
--threads 3 \
-np 1 \
--batch-size 512 \
--ubatch-size 512 \
--ctx-size 32768 \
--cache-type-k q8_0 \
--cache-type-v q8_0 \
--jinja \
--temp 0.6 --min-p 0.00 --top-p 0.95 --top-k 20 --repeat-penalty 1.0 --presence_penalty 1.0 \
--flash-attn on \
-a qwen3-thinking-30-a3b-8gb \
--n-gpu-layers 49 \
--override-tensor ".*blk\.([3-9]|[1-3][0-9]|4[0-8])\.ffn_.*_exps.*=CPU" \
--host 0.0.0.0 \
--port 3004

# Restart the service if it crashes or stops
Restart=always
# Wait 5 seconds before restarting
RestartSec=5

[Install]
# Service should be started when the system reaches multi-user target
WantedBy=multi-user.target
