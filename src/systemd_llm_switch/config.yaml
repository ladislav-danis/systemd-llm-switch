# Basic proxy server settings
server:
  host: "0.0.0.0"  # Address at which the server listens
  port: 3002         # Port on which the server will run
  llama_url: "http://localhost:3004" # Address of the llama.cpp server backend

# List of available models and their systemd services
# Simply add a new model as an additional line.
models:
  qwen3-coder-30-a3b-8gb: "qwen3-coder.service"
  qwen3-thinking-30-a3b-8gb: "qwen3-thinking.service"
  qwen3-coder-80-a3b-8gb: "qwen3-coder-next.service"
  qwen3-thinking-80-a3b-8gb: "qwen3-thinking-next.service"
  bge-m3: "bge-embedding.service"

  # Example of adding another model:
  # mistral-7b-v0.2-instruct: "mistral-instruct.service"