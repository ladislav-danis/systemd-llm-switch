# Basic proxy server settings
server:
  host: "0.0.0.0"  # Address at which the server listens
  port: 3002         # Port on which the server will run
  llama_url: "http://127.0.0.1:3004" # Address of the llama.cpp server backend
  trace_log: "llm_trace.log"  # File to log detailed input/output for debugging

# List of available models and their systemd services
# Simply add a new model as an additional line.
models:
  qwen3-thinking-next: "qwen3-thinking-next.service"
  qwen3-coder-next: "qwen3-coder-next.service"
  bge-m3: "bge-m3.service"

  # Example of adding another model:
  # mistral-7b-v0.2-instruct: "mistral-instruct.service"
  # qwen3-coder-flash: "qwen3-coder-flash.service"
  # qwen3-thinking-30-a3b-8gb: "qwen3-thinking.service"